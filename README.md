# ğŸš• NYC Taxi Demand Forecasting (CRISP-DM Case Study)

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange)
![MachineLearning](https://img.shields.io/badge/Machine%20Learning-RF%2FLightGBM%2FLSTM-green)
![DeepLearning](https://img.shields.io/badge/Deep%20Learning-Conv1D%20%2B%20LSTM-purple)
![GeoPandas](https://img.shields.io/badge/GeoSpatial-GeoPandas%2FShapely-lightgrey)

This repository contains an end-to-end forecasting pipeline for **hourly taxi demand** in New York City community districts.  
The project combines:

- Classical regression (Linear, Ridge, Lasso)
- Tree-based ensembles (Random Forest, LightGBM)
- A deep **Conv1D + LSTM** model
- A **Hybrid Ensemble** that blends all three families

The work follows the **CRISP-DM** methodology from business understanding to deployment-style evaluation and mapping.

---

## 1.  ğŸ“ Project Structure

```text
NYC_taxidemand_forecast/
â”œâ”€ data/
â”‚  â”œâ”€ raw/                  # Original source data 
â”‚  â”œâ”€ cleaned/              # Cleaned CSVs 
â”‚  â”œâ”€ processed/            # Datasets with additional features, train/test splits 
â”‚  â”œâ”€ runs/                 # Model artifacts and predictions 
â”‚  â””â”€ evaluation/           # Summary metrics, Map outputs
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ data/
â”‚  â”‚  â”œâ”€ clean_data.py          # Basic cleaning and geospatial filtering
â”‚  â”‚  â”œâ”€ preprocess.py          # Spatial join + hourly aggregation + features
â”‚  â”‚  â”œâ”€ split_data.py          # Chronological train/test split + scaling
â”‚  â”‚  â”œâ”€ data_prep.py           # Sequence preparation for Conv1D + LSTM
â”‚  â”‚  â””â”€ fetch_data_api.py      # (Optional) Download / load raw data
â”‚  â”‚
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ modeling_districts.py      # Linear models + RF + LightGBM per district
â”‚  â”‚  â”œâ”€ modeling_conv_lstm_train.py# Conv1D + LSTM per district
â”‚  â”‚  â””â”€ blend_hybrid.py            # Hybrid blending (RF + LGBM + ConvLSTM)
â”‚  â”‚
â”‚  â”œâ”€ evaluation/
â”‚  â”‚  â””â”€ evaluate_test_models.py    # Unified evaluation on final test set
â”‚  â”‚
â”‚  â”œâ”€ vizualize/
â”‚  â”‚  â””â”€ next_hour_map.py           # Folium map of next-hour forecast + RMSE
â”‚  â”‚
â”‚  â”œâ”€ utils/
â”‚  â”‚  â””â”€ pipeline_logger.py         # Centralized logging + pipeline step helper
â”‚  â”‚
â”‚  â””â”€ run_pipeline.py               # Model pipeline
â”‚
â”œâ”€ notebooks/
â”‚  â””â”€ figures                     # Result plots
â”‚  â””â”€ 00_analysis.ipynb           # Analysis of model results
â”‚  â””â”€ 01_exploration.ipynb        # Exploratory analysis 
â”‚  â””â”€ 02_modeling_baseline.ipynb  # Analysis of baseline model
â”‚
â”œâ”€ reports/                       # (Optional) Technical reports for Stakeholders
â”‚
â”œâ”€ README.md
â”œâ”€ requirements.txt
â””â”€ .gitignore
```

---
## 2. ğŸ’» Setup
### 2.1. Setup Create and activate a virtual environment (recommended)
```shell
cd NYC_taxidemand_forecast

python -m venv .venv

# Windows:
.venv\Scripts\activate

# macOS / Linux:
source .venv/bin/activate
```

### 2.2. Install dependencies
```shell
pip install --upgrade pip
pip install -r requirements.txt
```
You will also need system dependencies for GeoPandas (GDAL/GEOS/PROJ).
On Windows this is usually handled automatically when installing via pip or conda.


---
## 3. ğŸš– Data
The project specifically uses NYC district boundaries and pickup data from 2014. 

All geospatial joins use NYC community district polygons; these are not included in the repository for size/licensing reasons 
and must be added manually under data/raw/district_nyc_data/.

```shell
data/raw/district_nyc_data
data/raw/pickup_data
```

For the pickup data, the project uses publicly available data from:

**Taxi Pickup Records**  
Kaggle. (2019). *Uber Dataset from April to September 2014.*  
Dataset available at:  
https://www.kaggle.com/datasets/amirmotefaker/uber-dataset-from-april-to-september-2014

Feel free to fork, adapt, or extend the pipeline for other cities, additional features (e.g. weather, events), or alternative model architectures.

---
## 4. âš™ï¸ Run Pipeline
The entire modeling workflow can be launched with **one single command**:

```shell
python -m run_pipeline
```

This command executes:

ğŸ§¹ Data preprocessing (src/data/preprocess.py)

ğŸ”ª Train/test splitting + scaling (src/data/split_data.py)

â±ï¸ ConvLSTM sequence preparation (src/data/data_prep.py)

ğŸŒ³ Tree-based model training (src/models/modeling_districts.py)

ğŸ§  Conv1D + LSTM model training (src/models/modeling_conv_lstm_train.py)

âš–ï¸ Hybrid blending of all predictions (src/models/blend_hybrid.py)

âŒ Unified evaluation on the test set (src/evaluation/evaluate_test_models.py)

ğŸŒ Spatial visualization (Folium map)

ğŸ“Š All artifacts (predictions, logs, summaries, plots) are stored under:
```shell
data/runs/<timestamp>/
data/evaluation/
logs/
pipeline_log.csv
```
