{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 01_exploration.ipynb - Data Exploration",
   "id": "d5a825178b0710a0"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define base paths\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Raw data folder\n",
    "DATA_RAW = os.path.join(BASE_DIR, \"data\", \"raw\", \"pickup_data\")\n",
    "\n",
    "#  Figures folder\n",
    "FIG_DIR = os.path.abspath(os.path.join(os.getcwd(), \"figures\"))\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Figures will be saved to: {FIG_DIR}\")\n",
    "\n",
    "# Load and combine all monthly CSV file\n",
    "all_files = glob.glob(os.path.join(DATA_RAW, \"*.csv\"))\n",
    "print(f\"üîç Found {len(all_files)} CSV file(s) in {DATA_RAW}\")\n",
    "print(all_files)\n",
    "\n",
    "if all_files:\n",
    "    df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "    print(f\"‚úÖ Combined dataset shape: {df.shape}\")\n",
    "    display(df.head(10))\n",
    "    df.info()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No CSV files found. Please check the data/raw/pickup_data folder path.\")"
   ],
   "id": "ecba25bfc61becc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2ca1e52-2845-4eee-ba6f-2020b4828ad6",
   "metadata": {},
   "source": "## I.  Data Cleaning"
  },
  {
   "cell_type": "code",
   "id": "c72963d9-1bb4-406b-964d-e2766ce408d5",
   "metadata": {},
   "source": [
    "# Count true duplicates:Date/Time, Lat, Lon, and Base are exactly the same\n",
    "num_duplicates = df.duplicated(subset=['Date/Time','Lat','Lon','Base']).sum()\n",
    "print(f\"Number of duplicates based on original columns: {num_duplicates}\")\n",
    "\n",
    "# Check duplicates\n",
    "duplicate_rows = df[df.duplicated(subset=['Date/Time','Lat','Lon','Base'], keep=False)]\n",
    "duplicate_rows.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f2626f5-20f6-471f-8077-8051acf0a028",
   "metadata": {},
   "source": [
    "# Remove duplicates \n",
    "df = df.drop_duplicates(subset=['Date/Time','Lat','Lon','Base'])\n",
    "num_duplicates_after = df.duplicated(subset=['Date/Time','Lat','Lon','Base']).sum()\n",
    "print(f\"Remaining duplicates: {num_duplicates_after}\")\n",
    "\n",
    "# Check for missing values in each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# Quick check\n",
    "print(df.info())\n",
    "print(df.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97e7aaa4-3558-4743-bfc1-3fa546bcfd93",
   "metadata": {},
   "source": "## II. Data Exploration"
  },
  {
   "cell_type": "markdown",
   "id": "e7765fcc-ff7a-4cc6-b026-c97286a702c5",
   "metadata": {},
   "source": "#### Initial feature extraction"
  },
  {
   "cell_type": "code",
   "id": "4f49beb3-fded-48b8-8957-6a78e61c6487",
   "metadata": {},
   "source": [
    "# Convert Date/Time\n",
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "df['Hour'] = df['Date/Time'].dt.hour\n",
    "df['Day'] = df['Date/Time'].dt.day\n",
    "df['Weekday'] = df['Date/Time'].dt.weekday  # Monday=0, Sunday=6\n",
    "df['Month'] = df['Date/Time'].dt.month  # 1 = Jan, 12 = Dec"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f524870-76e0-4c6c-b5fa-4a4b84278f40",
   "metadata": {},
   "source": "### A. Temporal Data"
  },
  {
   "cell_type": "code",
   "id": "57edeef6-5c6a-4615-985b-431186c7b46d",
   "metadata": {},
   "source": [
    "# Get time range\n",
    "start_date = df['Date/Time'].min()\n",
    "end_date = df['Date/Time'].max()\n",
    "print(f\"üóìÔ∏è Date range: {start_date} ‚Üí {end_date}\")\n",
    "\n",
    "# Check number of days covered\n",
    "days_covered = (end_date - start_date).days\n",
    "print(f\"Total days covered: {days_covered}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77d88751-8ac3-4e0e-bb50-0fb60a9656c9",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "\n",
    "# Pickups per hour\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x='Hour', data=df, color='skyblue')\n",
    "plt.title('Uber Pickups by Hour of the Day')\n",
    "plt.xlabel('Hour of the Day (0‚Äì23)')\n",
    "plt.ylabel('Number of Pickups')\n",
    "plt.xticks(range(0,24))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Pickups per weekday\n",
    "weekday_labels = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x='Weekday', data=df, color='lightgreen')\n",
    "plt.title('Uber Pickups by Weekday')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Number of Pickups')\n",
    "plt.xticks(ticks=range(7), labels=weekday_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Average pickups per calendar day (across months)\n",
    "calendar_day = df.groupby('Day').size().reset_index(name='Total_Pickups')\n",
    "calendar_day['Avg_Pickups'] = calendar_day['Total_Pickups'] / df['Month'].nunique()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x='Day', y='Avg_Pickups', data=calendar_day, color='skyblue')\n",
    "plt.title(\"Average Pickups per Calendar Day (Across Months)\")\n",
    "plt.xlabel(\"Day of Month\")\n",
    "plt.ylabel(\"Average Number of Pickups\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Daily total pickups over time\n",
    "calendar_day_full = df.groupby(df['Date/Time'].dt.date).size().reset_index(name='Pickups')\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.lineplot(x='Date/Time', y='Pickups', data=calendar_day_full, color='steelblue')\n",
    "plt.title(\"Daily Total Pickups Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Pickups\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap: Hour vs Weekday\n",
    "pivot = df.pivot_table(index='Hour', columns='Weekday', values='Date/Time', aggfunc='count', fill_value=0)\n",
    "\n",
    "# Normalize within each weekday (column) to show % of daily pickups\n",
    "pivot_norm = pivot.div(pivot.sum(axis=0), axis=1) * 100  # convert to percentages\n",
    "\n",
    "# Rename columns to weekday names\n",
    "weekday_labels = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\n",
    "pivot_norm.columns = weekday_labels\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(\n",
    "    pivot_norm,\n",
    "    cmap=\"YlOrRd\",\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Percentage of Daily Pickups (%)'}\n",
    ")\n",
    "plt.title(\"Normalized Heatmap of Uber Pickups: Hour vs Weekday\", fontsize=14, pad=12)\n",
    "plt.xlabel(\"Weekday\", fontsize=12)\n",
    "plt.ylabel(\"Hour of Day\", fontsize=12)\n",
    "plt.yticks(rotation=0)  # keep hour labels horizontal\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Most frequent pickup patterns\n",
    "most_common_hour = df['Hour'].mode()[0]\n",
    "pickup_count_hour = df['Hour'].value_counts().head(1)\n",
    "print(\"Most frequent pickup hour:\")\n",
    "print(pickup_count_hour, '\\n')\n",
    "\n",
    "most_common_day = df['Day'].mode()[0]\n",
    "pickup_count_day = df['Day'].value_counts().head(1)\n",
    "print(\"Most frequent pickup day (calendar day of month):\")\n",
    "print(pickup_count_day, '\\n')\n",
    "\n",
    "most_common_weekday = df['Weekday'].mode()[0]\n",
    "pickup_count_weekday = df['Weekday'].value_counts().head(1)\n",
    "print(\"Most frequent weekday (0=Monday, 6=Sunday):\")\n",
    "print(pickup_count_weekday)\n",
    "print(f\"That‚Äôs a {calendar.day_name[most_common_weekday]}.\\n\")\n",
    "\n",
    "# Most frequent (hour, day) combination\n",
    "most_common_hour_day = df.groupby(['Day', 'Hour']).size().reset_index(name='Count')\n",
    "top_hour_day = most_common_hour_day.sort_values(by='Count', ascending=False).head(1)\n",
    "print(\"Most frequent hour‚Äìday combination:\")\n",
    "print(top_hour_day)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e73bd460-46fd-4dbe-9e48-5d1201185ca5",
   "metadata": {},
   "source": [
    "Hourly Pattern: A clear diurnal rhythm is visible, with minimal activity during early morning hours (2‚Äì5 AM) and distinct peaks during the evening commute (16:00‚Äì19:00). This pattern reflects strong commuter-driven demand and emphasizes the city‚Äôs daily mobility cycles.\n",
    "\n",
    "Weekly Pattern: Demand remains relatively stable during the weekday, with a noticeable rise on Thursday and Friday evenings as social and leisure activities increase. Weekends show later but sustained evening peaks, indicating different behavioral patterns compared to weekdays.\n",
    "\n",
    "Long-Term Trend: Over time, a steady upward trend is observed in overall demand, suggesting either service expansion, urban growth, change of profile of customers, or seasonal improvements (e.g., tourism or weather-related effects). This progression underscores the evolving nature of urban mobility."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Temporal predictors such as hour of day, weekday, and overall trend show the strongest explanatory power for short-term demand forecasting. In contrast, day-of-month adds little predictive value and can be safely deprioritized in modeling pipelines.",
   "id": "5dbfad75d8e5d003"
  },
  {
   "cell_type": "markdown",
   "id": "0093330e-c5ab-4c93-8097-a2edb870d892",
   "metadata": {},
   "source": "### B. Spatial Data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Outliers",
   "id": "bb3d5c41cb93c2f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Explore coordinate range\n",
    "lat_min, lat_max = df['Lat'].min(), df['Lat'].max()\n",
    "lon_min, lon_max = df['Lon'].min(), df['Lon'].max()\n",
    "\n",
    "print(f\"üìç Latitude range:  {lat_min:.4f} ‚Üí {lat_max:.4f}\")\n",
    "print(f\"üìç Longitude range: {lon_min:.4f} ‚Üí {lon_max:.4f}\")\n",
    "\n",
    "# Geographical center of all pickup points\n",
    "print(f\"üìå Average latitude:  {df['Lat'].mean():.4f}\")\n",
    "print(f\"üìå Average longitude: {df['Lon'].mean():.4f}\")\n",
    "\n",
    "# Define expected NYC bounds (rough operational area)\n",
    "lat_bounds = (40.5, 41.0)\n",
    "lon_bounds = (-74.3, -73.7)\n",
    "\n",
    "# - Find points outside NYC bounding box\n",
    "outliers_bounds = df[\n",
    "    (df['Lat'] < lat_bounds[0]) | (df['Lat'] > lat_bounds[1]) |\n",
    "    (df['Lon'] < lon_bounds[0]) | (df['Lon'] > lon_bounds[1])\n",
    "]\n",
    "print(f\"üöß Outliers outside NYC bounds: {len(outliers_bounds)}\")\n",
    "\n",
    "# Detect outliers more robustly using IQR (Interquartile Range)\n",
    "Q1_lat, Q3_lat = df['Lat'].quantile([0.25, 0.75])\n",
    "Q1_lon, Q3_lon = df['Lon'].quantile([0.25, 0.75])\n",
    "\n",
    "IQR_lat = Q3_lat - Q1_lat\n",
    "IQR_lon = Q3_lon - Q1_lon\n",
    "\n",
    "lat_min_iqr, lat_max_iqr = Q1_lat - 1.5 * IQR_lat, Q3_lat + 1.5 * IQR_lat\n",
    "lon_min_iqr, lon_max_iqr = Q1_lon - 1.5 * IQR_lon, Q3_lon + 1.5 * IQR_lon\n",
    "\n",
    "# Filter outliers based on IQR\n",
    "outliers_iqr = df[\n",
    "    (df['Lat'] < lat_min_iqr) | (df['Lat'] > lat_max_iqr) |\n",
    "    (df['Lon'] < lon_min_iqr) | (df['Lon'] > lon_max_iqr)\n",
    "]\n",
    "\n",
    "print(f\"üìâ Outliers detected via IQR: {len(outliers_iqr)}\")\n",
    "\n",
    "# -Filtered dataset (within NYC bounds and IQR range)\n",
    "nyc_df = df[\n",
    "    (df['Lat'].between(lat_min_iqr, lat_max_iqr)) &\n",
    "    (df['Lon'].between(lon_min_iqr, lon_max_iqr))\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Remaining NYC data points: {len(nyc_df)} ({len(nyc_df)/len(df)*100:.2f}% of total)\")\n"
   ],
   "id": "eaad0f486e12508c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4373a716-c23b-4abb-b321-07a7ee41d7cd",
   "metadata": {},
   "source": [
    "The dataset covers pickup coordinates confirm that nearly all trips fall within the New York City metropolitan area. The average pickup location (40.739¬∞, ‚àí73.973¬∞) aligns with central Manhattan, highlighting strong urban concentration.\n",
    "\n",
    "A small share of trips (~0.6%) occurred outside city bounds, while the IQR-based filter flagged an additional ~11% of statistically extreme points likely caused by long-distance rides beyond operational relevance.\n",
    "\n",
    "After cleaning, 3.94 million valid pickups (‚âà88.6%) remain, providing a robust and geographically consistent dataset for subsequent clustering and demand forecasting."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Top 10 Busiest Pickup Locations (All Days, All Hours)",
   "id": "8611934500016c08"
  },
  {
   "cell_type": "code",
   "id": "04fd1383-cfed-4fad-850e-c8efebff7045",
   "metadata": {},
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "nyc_df['Lat_round'] = nyc_df['Lat'].round(3)\n",
    "nyc_df['Lon_round'] = nyc_df['Lon'].round(3)\n",
    "\n",
    "# Find the Top 10 busiest pickup points\n",
    "top_locations = (\n",
    "    nyc_df.groupby(['Lat_round', 'Lon_round'])\n",
    "          .size()\n",
    "          .reset_index(name='Count')\n",
    "          .sort_values(by='Count', ascending=False)\n",
    "          .head(10)\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Reverse Geocode the coordinates\n",
    "geolocator = Nominatim(user_agent=\"nyc_taxi_analysis\")\n",
    "top_locations['Address'] = None  # prepare empty column\n",
    "\n",
    "for i, row in top_locations.iterrows():\n",
    "    try:\n",
    "        location = geolocator.reverse((row['Lat_round'], row['Lon_round']), exactly_one=True, timeout=10)\n",
    "        top_locations.at[i, 'Address'] = location.address if location else \"Unknown\"\n",
    "        time.sleep(1)  # prevent API blocking by Nominatim\n",
    "    except Exception as e:\n",
    "        top_locations.at[i, 'Address'] = f\"Error: {e}\"\n",
    "\n",
    "\n",
    "display(top_locations[['Lat_round', 'Lon_round', 'Count', 'Address']])\n",
    "\n",
    "# Interactive Folium Map\n",
    "nyc_center = [40.7128, -74.0060]\n",
    "m = folium.Map(location=nyc_center, zoom_start=11, tiles='CartoDB positron')\n",
    "\n",
    "# Add each top location as a circle marker\n",
    "for _, row in top_locations.iterrows():\n",
    "    popup_text = f\"\"\"\n",
    "    <b>Pickups:</b> {row['Count']}<br>\n",
    "    <b>Coordinates:</b> ({row['Lat_round']:.4f}, {row['Lon_round']:.4f})<br>\n",
    "    <b>Address:</b> {row['Address']}\n",
    "    \"\"\"\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Lat_round'], row['Lon_round']],\n",
    "        radius=6 + (row['Count'] / top_locations['Count'].max()) * 10,  # scaled radius\n",
    "        color='navy',\n",
    "        fill=True,\n",
    "        fill_color='lightblue',\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(popup_text, max_width=300)\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save map to the Figures folder\n",
    "map_path = os.path.join(FIG_DIR, \"01_top10_hotspots.html\")\n",
    "m.save(map_path)\n",
    "\n",
    "print(f\"üíæ Map saved to: {map_path}\")\n",
    "\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e4c4268-a76f-42cb-99a3-7bf57b3c9b72",
   "metadata": {},
   "source": "Spatial exploration after removing outliers (e.g. airport pickups) revealed that the majority of high-demand locations are primarily around Manhattan‚Äôs commercial and entertainment zones such as Chelsea, Midtown, and SoHo."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spatial Clustering on NYC Pickup Points",
   "id": "c2d3427b656a12cf"
  },
  {
   "cell_type": "code",
   "id": "a0815d32-4cd9-4179-8a1c-c2d4a74841c2",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Sample data for faster computation\n",
    "sample_size = 100_000  # adjust if still slow\n",
    "df_sample = nyc_df.sample(n=sample_size, random_state=42)[['Lat', 'Lon']]\n",
    "\n",
    "print(f\"Running elbow method on {len(df_sample):,} points...\")\n",
    "\n",
    "# Compute inertia for different K values\n",
    "inertia = []\n",
    "K = range(3, 16)  # test cluster numbers between 3 and 15\n",
    "\n",
    "for k in K:\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=10_000, random_state=42)\n",
    "    kmeans.fit(df_sample)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K, inertia, 'bo-', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "plt.title('Elbow Method to Determine Optimal k', fontsize=14, pad=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5fab9d79-f75e-4f49-9303-09aa4d77d8ff",
   "metadata": {},
   "source": "The elbow plot indicates a clear inflection point around k = 7‚Äì8, after which the rate of decrease in within-cluster variance slows. This suggests that 7‚Äì8 clusters would sufficiently represent the main spatial patterns in the data. Beyond this point, adding more clusters yields only marginal improvements while increasing model complexity."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Kmeans model for 10 clusters",
   "id": "3025ed88bb617e7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# KMeans Clustering (MiniBatch for speed)\n",
    "coords = nyc_df[['Lat', 'Lon']]\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=10, batch_size=10_000, random_state=42)\n",
    "kmeans.fit(coords)\n",
    "\n",
    "# Assign clusters\n",
    "nyc_df['Cluster'] = kmeans.predict(coords)\n",
    "\n",
    "# Cluster centroids (from model)\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=['Centroid_Lat', 'Centroid_Lon'])\n",
    "centroids['Cluster'] = range(10)\n",
    "\n",
    "# - Summary stats per cluster\n",
    "cluster_summary = (\n",
    "    nyc_df.groupby('Cluster')\n",
    "    .agg(\n",
    "        Pickups=('Cluster', 'size'),\n",
    "        Avg_Lat=('Lat', 'mean'),\n",
    "        Avg_Lon=('Lon', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge centroid and summary info\n",
    "kmeans_summary = pd.merge(centroids, cluster_summary, on='Cluster', how='left')\n",
    "\n",
    "# Add percentage of total pickups\n",
    "total_pickups = len(nyc_df)\n",
    "kmeans_summary['Pct_of_Total'] = (kmeans_summary['Pickups'] / total_pickups * 100).round(2)\n",
    "\n",
    "# Compute cluster radius (approximate geographic spread)\n",
    "def cluster_radius(cluster_id):\n",
    "    cluster_id = int(cluster_id)  # ensure integer type\n",
    "    cluster_points = nyc_df.loc[nyc_df['Cluster'] == cluster_id, ['Lat', 'Lon']]\n",
    "    centroid = np.array([\n",
    "        kmeans_summary.loc[kmeans_summary['Cluster'] == cluster_id, 'Centroid_Lat'].values[0],\n",
    "        kmeans_summary.loc[kmeans_summary['Cluster'] == cluster_id, 'Centroid_Lon'].values[0]\n",
    "    ])\n",
    "    distances = np.sqrt(((cluster_points - centroid) ** 2).sum(axis=1))\n",
    "    return np.mean(distances)\n",
    "\n",
    "kmeans_summary['Cluster'] = kmeans_summary['Cluster'].astype(int)\n",
    "kmeans_summary['Radius'] = kmeans_summary['Cluster'].apply(cluster_radius).round(4)\n",
    "\n",
    "kmeans_summary = kmeans_summary[['Cluster','Pickups','Pct_of_Total','Avg_Lat','Avg_Lon','Radius']]\n",
    "\n",
    "display(kmeans_summary)\n",
    "\n",
    "#  Visualize clusters on a Folium map ---\n",
    "nyc_center = [40.7128, -74.0060]\n",
    "m = folium.Map(location=nyc_center, zoom_start=11, tiles='CartoDB positron')\n",
    "\n",
    "colors = [\n",
    "    'red','blue','green','purple','orange',\n",
    "    'darkred','lightblue','darkgreen','gray','pink'\n",
    "]\n",
    "\n",
    "for _, row in kmeans_summary.iterrows():\n",
    "    cluster_id = int(row['Cluster'])  # make sure it's int\n",
    "    popup_text = f\"\"\"\n",
    "    <b>Cluster:</b> {cluster_id}<br>\n",
    "    <b>Pickups:</b> {row['Pickups']:,}<br>\n",
    "    <b>Share of Total:</b> {row['Pct_of_Total']}%<br>\n",
    "    <b>Avg Coordinates:</b> ({row['Avg_Lat']:.4f}, {row['Avg_Lon']:.4f})<br>\n",
    "    <b>Radius:</b> {row['Radius']:.4f}¬∞\n",
    "    \"\"\"\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Avg_Lat'], row['Avg_Lon']],\n",
    "        radius=8 + (row['Pickups'] / kmeans_summary['Pickups'].max()) * 8,\n",
    "        color=colors[cluster_id % len(colors)],\n",
    "        fill=True,\n",
    "        fill_color=colors[cluster_id % len(colors)],\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(popup_text, max_width=300)\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save map to the Figures folder\n",
    "k_map_path = os.path.join(FIG_DIR, \"01_kmeans_clusters_nyc.html\")\n",
    "m.save(k_map_path)\n",
    "\n",
    "print(f\"üíæ Map saved to: {k_map_path}\")\n",
    "\n",
    "m"
   ],
   "id": "1ab30ee47ff69c24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### NYC Community Districs",
   "id": "12d6fbf572c98693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While the results confirmed that demand is highly concentrated around central Manhattan, cluster boundaries proved unstable and uneven.\n",
    "To improve interpretability and align with operational planning, the analysis transitioned to New York City community districts as consistent geographic units for further demand modeling."
   ],
   "id": "32d5baf51cae680"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset included was taken from https://www.nyc.gov/content/planning/pages/resources/datasets/community-districts.",
   "id": "9bc4295663af23be"
  },
  {
   "cell_type": "code",
   "id": "022356b0-45b2-452c-8dfc-03f291590e3b",
   "metadata": {},
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(nyc_df['Lon'], nyc_df['Lat'])]\n",
    "gdf = gpd.GeoDataFrame(nyc_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "print(f\"‚úì Converted nyc_df to GeoDataFrame with {len(gdf):,} pickup points\")\n",
    "\n",
    "# Define dynamic path (from inside notebooks folder)\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_RAW = os.path.join(BASE_DIR, \"data\", \"raw\", \"district_nyc_data\")\n",
    "\n",
    "# Try loading a valid geographic file automatically\n",
    "try:\n",
    "    print(f\"\\nLooking for Community Districts files in: {DATA_RAW}\")\n",
    "\n",
    "    if not os.path.exists(DATA_RAW):\n",
    "        raise FileNotFoundError(f\"Path not found: {DATA_RAW}\")\n",
    "\n",
    "    geo_files = [\n",
    "        f for f in os.listdir(DATA_RAW)\n",
    "        if any(f.lower().endswith(ext) for ext in ['.shp', '.geojson', '.json', '.gpkg'])\n",
    "    ]\n",
    "\n",
    "    if geo_files:\n",
    "        file_path = os.path.join(DATA_RAW, geo_files[0])\n",
    "        nyc_cds = gpd.read_file(file_path)\n",
    "        print(f\"‚úì Loaded geographic file: {geo_files[0]}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No shapefile or GeoJSON found in data/raw/district_nyc_data\")\n",
    "\n",
    "    print(f\"Loaded {len(nyc_cds)} community district polygons\")\n",
    "    print(\"Available columns:\", list(nyc_cds.columns))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading local file: {e}\")\n",
    "    # --- Fallback: create synthetic 5x5 grid if file not found ---\n",
    "    from shapely.geometry import Polygon\n",
    "    import numpy as np\n",
    "\n",
    "    nyc_bounds = [-74.05, 40.55, -73.70, 40.95]\n",
    "    grid_cells, names = [], []\n",
    "    x_min, y_min, x_max, y_max = nyc_bounds\n",
    "    x_step = (x_max - x_min) / 5\n",
    "    y_step = (y_max - y_min) / 5\n",
    "\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            cell = Polygon([\n",
    "                (x_min + i * x_step, y_min + j * y_step),\n",
    "                (x_min + (i+1) * x_step, y_min + j * y_step),\n",
    "                (x_min + (i+1) * x_step, y_min + (j+1) * y_step),\n",
    "                (x_min + i * x_step, y_min + (j+1) * y_step)\n",
    "            ])\n",
    "            grid_cells.append(cell)\n",
    "            names.append(f\"Grid_{i}_{j}\")\n",
    "\n",
    "    nyc_cds = gpd.GeoDataFrame({'cd_name': names, 'geometry': grid_cells}, crs=\"EPSG:4326\")\n",
    "    print(\"‚úì Using fallback grid polygons instead of districts\")\n",
    "\n",
    "# Clean up CRS and naming\n",
    "nyc_cds = nyc_cds.to_crs(gdf.crs)\n",
    "\n",
    "if 'boro_cd' in nyc_cds.columns and 'boro_name' in nyc_cds.columns:\n",
    "    nyc_cds['cd_name'] = nyc_cds['boro_name'] + ' CD ' + nyc_cds['boro_cd'].astype(str).str[1:]\n",
    "elif 'boro_cd' in nyc_cds.columns:\n",
    "    borough_map = {'1': 'Manhattan', '2': 'Bronx', '3': 'Brooklyn', '4': 'Queens', '5': 'Staten Island'}\n",
    "    nyc_cds['cd_name'] = nyc_cds['boro_cd'].astype(str).apply(\n",
    "        lambda x: f\"{borough_map.get(x[0], 'Unknown')} CD {x[1:]}\" if len(x) == 3 else f\"CD_{x}\"\n",
    "    )\n",
    "else:\n",
    "    name_col = next((col for col in ['label', 'NAME', 'name', 'CommunityDistrict', 'ntaname', 'boro_name']\n",
    "                     if col in nyc_cds.columns), None)\n",
    "    nyc_cds['cd_name'] = nyc_cds[name_col] if name_col else [f'CD_{i}' for i in range(len(nyc_cds))]\n",
    "\n",
    "print(\"‚úì Community district names assigned\")\n",
    "\n",
    "# Spatial join\n",
    "print(\"\\nPerforming spatial join (this might take a few minutes)...\")\n",
    "gdf_with_cds = gpd.sjoin(gdf, nyc_cds[['cd_name', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "\n",
    "# -Calculate pickups per district\n",
    "pickup_stats = gdf_with_cds.groupby('cd_name').size().reset_index(name='count')\n",
    "pickup_stats['percentage'] = (pickup_stats['count'] / pickup_stats['count'].sum() * 100).round(2)\n",
    "top_10 = pickup_stats.nlargest(10, 'count')\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"TOP 10 COMMUNITY DISTRICTS BY TAXI PICKUPS\")\n",
    "print(\"=\"*65)\n",
    "for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['cd_name']:<35} {row['count']:>9,} pickups ({row['percentage']:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal pickups analyzed: {pickup_stats['count'].sum():,}\")\n",
    "print(f\"Top 10 districts cover {top_10['percentage'].sum():.2f}% of all pickups\")\n",
    "\n",
    "# Borough summary\n",
    "if 'boro_name' in nyc_cds.columns:\n",
    "    cd_to_borough = nyc_cds.set_index('cd_name')['boro_name'].to_dict()\n",
    "    pickup_stats['borough'] = pickup_stats['cd_name'].map(cd_to_borough)\n",
    "    borough_summary = (\n",
    "        pickup_stats.groupby('borough')\n",
    "        .agg({'count': 'sum', 'percentage': 'sum'})\n",
    "        .sort_values('count', ascending=False)\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PICKUPS BY BOROUGH\")\n",
    "    print(\"=\"*50)\n",
    "    print(borough_summary)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c8adfba-df7f-4f82-9851-87f8652956cc",
   "metadata": {},
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "nyc_cds_map = nyc_cds.merge(pickup_stats, on=\"cd_name\", how=\"left\")\n",
    "\n",
    "# Drop datetime-like columns (explicitly) and reset index to remove datetime index metadata\n",
    "for col in nyc_cds_map.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(nyc_cds_map[col]):\n",
    "        nyc_cds_map[col] = nyc_cds_map[col].astype(str)\n",
    "\n",
    "nyc_cds_map = nyc_cds_map.reset_index(drop=True)\n",
    "\n",
    "# Create base map\n",
    "m = folium.Map(location=[40.75, -73.98], zoom_start=11, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Choropleth layer\n",
    "folium.Choropleth(\n",
    "    geo_data=nyc_cds_map.to_json(),\n",
    "    data=nyc_cds_map,\n",
    "    columns=[\"cd_name\", \"count\"],\n",
    "    key_on=\"feature.properties.cd_name\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.4,\n",
    "    legend_name=\"Taxi Pickups per Community District\"\n",
    ").add_to(m)\n",
    "\n",
    "# Add top 10 district markers\n",
    "for _, row in nyc_cds_map[nyc_cds_map[\"cd_name\"].isin(top_10[\"cd_name\"])].iterrows():\n",
    "    centroid = row[\"geometry\"].centroid\n",
    "    popup_text = f\"\"\"\n",
    "    <b>{row['cd_name']}</b><br>\n",
    "    {int(row['count']):,} pickups<br>\n",
    "    ({row['percentage']:.1f}% of total)\n",
    "    \"\"\"\n",
    "    folium.CircleMarker(\n",
    "        location=[centroid.y, centroid.x],\n",
    "        radius=7,\n",
    "        color=\"black\",\n",
    "        fill=True,\n",
    "        fill_color=\"red\",\n",
    "        fill_opacity=0.8,\n",
    "        popup=folium.Popup(popup_text, max_width=300)\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save map to the Figures folder\n",
    "(d_map_path) = os.path.join(FIG_DIR, \"01_nyc_pickups_by_district.html\")\n",
    "m.save(d_map_path)\n",
    "\n",
    "print(f\"üíæ Map saved to: {d_map_path}\")\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The choropleth map highlights strong spatial concentration of taxi pickups within Manhattan‚Äôs central and western districts, particularly around Midtown, Chelsea, and Lower Manhattan.\n",
    "\n",
    "These districts display the highest pickup volumes, driven by dense commercial activity, tourism, and transport hubs such as Penn Station and the Financial District.\n",
    "Demand gradually decreases toward outer boroughs, where residential areas and lower population density reduce ride frequency.\n",
    "\n",
    "The top 10 community districts account for a majority of all rides, confirming that taxi demand is highly localized and uneven across NYC.\n",
    "This insight supports the need for district-level forecasting for operational efficiency."
   ],
   "id": "ac4c310dbc3894af"
  },
  {
   "cell_type": "markdown",
   "id": "6ba16373-0820-4273-84cf-6f426758edf9",
   "metadata": {},
   "source": "### C. Spatio-Temporal Exploration"
  },
  {
   "cell_type": "code",
   "id": "0de1e7bb-07e7-4bfa-8adc-d0416aa7fa92",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top 10 districts\n",
    "top_10 = gdf_with_cds.groupby('cd_name').size().reset_index(name='count').nlargest(10, 'count')\n",
    "df_top10 = gdf_with_cds[gdf_with_cds['cd_name'].isin(top_10['cd_name'])].copy()\n",
    "\n",
    "# Hour vs Top 10 districts\n",
    "hour_district = df_top10.groupby(['Hour','cd_name']).size().reset_index(name='pickups')\n",
    "hour_pivot = hour_district.pivot(index='Hour', columns='cd_name', values='pickups').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(hour_pivot, cmap='Reds', linewidths=.5, annot=True, fmt=\".0f\")\n",
    "plt.title('Average Pickups per Hour by Top 10 Districts')\n",
    "plt.xlabel('District')\n",
    "plt.ylabel('Hour of Day')\n",
    "plt.show()\n",
    "\n",
    "# Weekday vs Top 10 districts\n",
    "weekday_district = df_top10.groupby(['Weekday','cd_name']).size().reset_index(name='pickups')\n",
    "weekday_pivot = weekday_district.pivot(index='Weekday', columns='cd_name', values='pickups').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(weekday_pivot, cmap='Blues', linewidths=.5, annot=True, fmt=\".0f\")\n",
    "plt.title('Average Pickups per Weekday by Top 10 Districts')\n",
    "plt.xlabel('District')\n",
    "plt.ylabel('Weekday (0=Mon)')\n",
    "plt.show()\n",
    "\n",
    "# Month vs Top 10 districts\n",
    "month_district = df_top10.groupby(['Month','cd_name']).size().reset_index(name='pickups')\n",
    "month_pivot = month_district.pivot(index='Month', columns='cd_name', values='pickups').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(month_pivot, cmap='Oranges', linewidths=.5, annot=True, fmt=\".0f\")\n",
    "plt.title('Average Pickups per Month by Top 10 Districts')\n",
    "plt.xlabel('District')\n",
    "plt.ylabel('Month')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Combining hourly, weekly, and monthly trends reveals that NYC taxi demand is both spatially and temporally polarized. Manhattan‚Äôs commercial cores exhibit synchronized evening and midweek peaks, while Brooklyn‚Äôs districts contribute steady, moderate flows.\n",
    "\n",
    "Hourly Patterns: Taxi demand peaks sharply during evening hours (17:00‚Äì20:00) across all top districts, especially in Midtown and Lower Manhattan (CD 04‚Äì05).\n",
    "Early morning hours (2‚Äì5 AM) show minimal activity, reflecting clear commuter and nightlife-driven cycles.\n",
    "\n",
    "Weekly Patterns: Demand is highest on Tuesdays to Thursdays, tapering slightly over weekends.\n",
    "Manhattan districts dominate weekday activity, while Brooklyn CDs show steadier weekend demand linked to leisure travel.\n",
    "\n",
    "Monthly Patterns: Pickups increase from spring to early autumn, peaking around September.\n",
    "The strongest seasonal growth occurs in central Manhattan, consistent with tourism and business mobility trends."
   ],
   "id": "ed69e12c45447ed4"
  },
  {
   "cell_type": "code",
   "id": "776a5df4-447e-4785-a771-8e4737895a81",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import os\n",
    "\n",
    "# Top 10 districts\n",
    "top_10 = gdf_with_cds.groupby('cd_name').size().reset_index(name='count').nlargest(10, 'count')\n",
    "df_top10 = gdf_with_cds[gdf_with_cds['cd_name'].isin(top_10['cd_name'])].copy()\n",
    "\n",
    "# Create EDA tables\n",
    "hourly_table = (\n",
    "    df_top10.groupby(['cd_name', 'Hour'])\n",
    "    .size().reset_index(name='pickups')\n",
    "    .pivot(index='Hour', columns='cd_name', values='pickups').fillna(0)\n",
    ")\n",
    "\n",
    "weekday_table = (\n",
    "    df_top10.groupby(['cd_name', 'Weekday'])\n",
    "    .size().reset_index(name='pickups')\n",
    "    .pivot(index='Weekday', columns='cd_name', values='pickups').fillna(0)\n",
    ")\n",
    "\n",
    "day_table = (\n",
    "    df_top10.groupby(['cd_name', 'Day'])\n",
    "    .size().reset_index(name='pickups')\n",
    "    .pivot(index='Day', columns='cd_name', values='pickups').fillna(0)\n",
    ")\n",
    "\n",
    "month_table = (\n",
    "    df_top10.groupby(['cd_name', 'Month'])\n",
    "    .size().reset_index(name='pickups')\n",
    "    .pivot(index='Month', columns='cd_name', values='pickups').fillna(0)\n",
    ")\n",
    "\n",
    "# Correlation matrix with temporal features\n",
    "agg = df_top10.groupby(['Hour', 'Weekday', 'Day', 'Month', 'cd_name']).size().reset_index(name='pickups')\n",
    "pivot = agg.pivot_table(index=['Hour', 'Weekday', 'Day', 'Month'], columns='cd_name', values='pickups', fill_value=0)\n",
    "\n",
    "pivot['Hour'] = pivot.index.get_level_values('Hour')\n",
    "pivot['Weekday'] = pivot.index.get_level_values('Weekday')\n",
    "pivot['Day'] = pivot.index.get_level_values('Day')\n",
    "pivot['Month'] = pivot.index.get_level_values('Month')\n",
    "\n",
    "corr_table = pivot.corr()\n",
    "\n",
    "# Plot and save correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_table, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation of Pickups with Temporal Features\")\n",
    "\n",
    "corr_path = os.path.join(FIG_DIR, \"01_correlation_heatmap.png\")\n",
    "plt.savefig(corr_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"üíæ Correlation heatmap saved to: {corr_path}\")\n",
    "\n",
    "#  Save to the Figures folder\n",
    "hourly_table.to_csv(os.path.join(FIG_DIR, \"01_hourly_table.csv\"))\n",
    "weekday_table.to_csv(os.path.join(FIG_DIR, \"01_weekday_table.csv\"))\n",
    "day_table.to_csv(os.path.join(FIG_DIR, \"01_day_table.csv\"))\n",
    "month_table.to_csv(os.path.join(FIG_DIR, \"01_month_table.csv\"))\n",
    "corr_table.to_csv(os.path.join(FIG_DIR, \"01_correlation_table.csv\"))\n",
    "\n",
    "print(\"‚úÖ All tables and figures saved in:\", FIG_DIR)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5864692-39e4-441b-9aea-3c2a0da15606",
   "metadata": {},
   "source": [
    "The heatmap shows strong positive correlations among most Manhattan districts (r ‚âà 0.85‚Äì0.9) ‚Äî indicating that demand patterns across central Manhattan move together.\n",
    "\n",
    "Brooklyn districts display moderate correlations (r ‚âà 0.6‚Äì0.75) with each other and weaker links to Manhattan zones, reflecting distinct local demand dynamics.\n",
    "\n",
    "Temporal features like Hour show moderate correlations (r ‚âà 0.3‚Äì0.6) with Manhattan districts, which makes time-based cycles worth exploring in urban areas, while Weekday and Day contribute little direct variance.\n",
    "\n",
    "Overall, this suggests that spatial proximity can drive synchronized demand, while temporal variation mainly amplifies existing Manhattan-centric trends."
   ]
  },
  {
   "cell_type": "code",
   "id": "fbb1d84c-a2f2-4acd-83ca-ff8066e6dff4",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "# Aggregate hourly data\n",
    "time_series = (\n",
    "    df_top10.groupby(['cd_name', 'Hour', 'Weekday', 'Day', 'Month'])\n",
    "    .size()\n",
    "    .reset_index(name='pickups')\n",
    ")\n",
    "\n",
    "# Create a pseudo time index\n",
    "time_series['timestamp'] = (\n",
    "    time_series['Month'].astype(str).str.zfill(2) + '-' +\n",
    "    time_series['Day'].astype(str).str.zfill(2) + ' ' +\n",
    "    time_series['Hour'].astype(str).str.zfill(2)\n",
    ")\n",
    "time_series['timestamp'] = pd.to_datetime(time_series['timestamp'], format='%m-%d %H', errors='coerce')\n",
    "\n",
    "#  ACF & PACF calculation\n",
    "acf_results = {}\n",
    "pacf_results = {}\n",
    "max_lag = 48  # 2 days\n",
    "\n",
    "for district in top_10['cd_name']:\n",
    "    district_data = (\n",
    "        time_series[time_series['cd_name'] == district]\n",
    "        .sort_values('timestamp')\n",
    "        ['pickups']\n",
    "        .fillna(0)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "    if len(district_data) < max_lag:\n",
    "        print(f\"‚ö†Ô∏è Skipping {district}: insufficient data ({len(district_data)} points)\")\n",
    "        continue\n",
    "\n",
    "    acf_vals = acf(district_data, nlags=max_lag, fft=True)\n",
    "    pacf_vals = pacf(district_data, nlags=max_lag, method='yw')\n",
    "\n",
    "    acf_results[district] = acf_vals\n",
    "    pacf_results[district] = pacf_vals\n",
    "\n",
    "#  Convert to DataFrames\n",
    "lags = np.arange(0, max_lag + 1)\n",
    "acf_df = pd.DataFrame(acf_results, index=lags).reset_index().rename(columns={'index': 'Lag'})\n",
    "pacf_df = pd.DataFrame(pacf_results, index=lags).reset_index().rename(columns={'index': 'Lag'})\n",
    "\n",
    "# Export CSV\n",
    "acf_csv = os.path.join(FIG_DIR, \"01_acf_results_top10.csv\")\n",
    "pacf_csv = os.path.join(FIG_DIR, \"01_pacf_results_top10.csv\")\n",
    "\n",
    "print(\"‚úÖ Exported CSVs:\")\n",
    "print(\"01_acf_results_top10.csv\")\n",
    "print(\"01_pacf_results_top10.csv\")\n",
    "\n",
    "# Visualization\n",
    "num_districts = len(acf_results)\n",
    "fig, axes = plt.subplots(num_districts, 2, figsize=(12, 4 * num_districts))\n",
    "\n",
    "for i, district in enumerate(acf_results.keys()):\n",
    "    # Confidence bounds\n",
    "    N = len(time_series[time_series['cd_name'] == district])\n",
    "    conf = 1.96 / np.sqrt(N)\n",
    "\n",
    "    # ACF\n",
    "    axes[i, 0].stem(lags, acf_results[district], linefmt='b-', markerfmt='bo', basefmt='r-')\n",
    "    axes[i, 0].axhline(y=conf, color='r', linestyle='--', linewidth=0.8)\n",
    "    axes[i, 0].axhline(y=-conf, color='r', linestyle='--', linewidth=0.8)\n",
    "    axes[i, 0].set_title(f\"{district} ‚Äì ACF\", fontsize=11)\n",
    "    axes[i, 0].set_xlabel(\"Lag (hours)\")\n",
    "    axes[i, 0].set_ylabel(\"Correlation\")\n",
    "\n",
    "    # PACF\n",
    "    axes[i, 1].stem(lags, pacf_results[district], linefmt='g-', markerfmt='go', basefmt='r-')\n",
    "    axes[i, 1].axhline(y=conf, color='r', linestyle='--', linewidth=0.8)\n",
    "    axes[i, 1].axhline(y=-conf, color='r', linestyle='--', linewidth=0.8)\n",
    "    axes[i, 1].set_title(f\"{district} ‚Äì PACF\", fontsize=11)\n",
    "    axes[i, 1].set_xlabel(\"Lag (hours)\")\n",
    "    axes[i, 1].set_ylabel(\"Partial Correlation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to the Figures folder\n",
    "acf_pacf_path = os.path.join(FIG_DIR, \"01_acf_pacf_top10.png\")\n",
    "plt.savefig(acf_pacf_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"üíæ ACF/PACF plots saved to: {acf_pacf_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cab8b45-a8b2-4489-872c-48dd589c9a14",
   "metadata": {},
   "source": [
    "The ACF plots show strong short-lag autocorrelation across nearly all districts, confirming that current taxi demand is highly dependent on recent hours.\n",
    "Distinct spikes at lag = 24 indicate clear daily seasonality, reflecting the repeated 24-hour pickup cycle.\n",
    "\n",
    "PACF plots reinforce this pattern, with initial lags remaining significant before tapering off, suggesting that only a few recent time steps (typically within 1‚Äì3 hours) contribute most to predictive power.\n",
    "\n",
    "These findings validate the use of lag features and sequence-based models (e.g., LSTM, Conv1D) for hourly demand forecasting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Demand forecast",
   "language": "python",
   "name": "taxidemand_forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
